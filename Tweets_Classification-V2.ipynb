{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs, re, json\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.mllib.fpm import FPGrowth\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, IDF, StopWordsRemover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Create Spark and SQL context:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Text Classifier, master=local[*]) created by __init__ at <ipython-input-2-eea46be77db3>:3 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-eea46be77db3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Text Classifier\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# if not sc:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/opt/spark/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \"\"\"\n\u001b[0;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32m/opt/spark/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway)\u001b[0m\n\u001b[0;32m    259\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 261\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    262\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Text Classifier, master=local[*]) created by __init__ at <ipython-input-2-eea46be77db3>:3 "
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName(\"Text Classifier\")\n",
    "# if not sc:\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Parse Tweets to tweet_id and tweet_text:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parseTweet(line):\n",
    "    \"\"\"\n",
    "    Parses a tweet record having the following format collectionId-tweetId<\\t>tweetString\n",
    "    \"\"\"\n",
    "    fields = line.strip().split(\"\\t\")\n",
    "    if len(fields) == 2:\n",
    "        # The following regex just strips of an URL (not just http), any punctuations, \n",
    "        # User Names or Any non alphanumeric characters\n",
    "        # http://goo.gl/J8ZxDT\n",
    "        text = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",fields[1]).strip()\n",
    "        # remove terms <= 2 characters\n",
    "        text = ' '.join(filter(lambda x: len(x) > 2, text.split(\" \")))\n",
    "        # return tuple of (collectionId-tweetId, text)\n",
    "        return (fields[0], text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadConfig(config_file):\n",
    "    \"\"\"\n",
    "    Load collection configuration file.\n",
    "    \"\"\"\n",
    "    with open(config_file) as data_file:    \n",
    "        config_data = json.load(data_file)\n",
    "    return config_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_dir = \"/home/hshahin/spring2016_ir_project/data/\"\n",
    "FP_dir = base_dir + \"FPGrowth/\"\n",
    "config_file = \"collections_config.json\"\n",
    "config_data = loadConfig(base_dir + config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Load tweets from file into DataFrame:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection_id = config_data[\"collections\"][0][\"Id\"]\n",
    "tweets_file = base_dir + \"small_data/z_\" + collection_id\n",
    "tweets = sc.textFile(tweets_file) \\\n",
    "          .map(parseTweet) \\\n",
    "          .filter(lambda x: x is not None) \\\n",
    "          .map(lambda x: Row(id=x[0], text=x[1])) \\\n",
    "          .toDF() \\\n",
    "          .cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h1>Tokenize and remove stop words from tweet text:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "tweets = tokenizer.transform(tweets)\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "tweets = remover.transform(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Save unique tokens:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = (tweets\n",
    "  .rdd\n",
    "  .map(lambda x : (x.id, x.text, list(set(filter(None, x.filtered)))))\n",
    "  .toDF()\n",
    "  .withColumnRenamed(\"_1\",\"id\")\n",
    "  .withColumnRenamed(\"_2\",\"text\")\n",
    "  .withColumnRenamed(\"_3\",\"filtered\")).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Run Frequent Pattern Mining algorithm and save to output file:</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = FPGrowth.train(tweets.select(\"filtered\").rdd.map(lambda x: x[0]), minSupport=0.02)\n",
    "result = sorted(model.freqItemsets().collect(), reverse=True)\n",
    "# sort the result in reverse order\n",
    "sorted_result = sorted(result, key=lambda item: int(item.freq), reverse=True)\n",
    "\n",
    "# save output to file\n",
    "with codecs.open(FP_dir + collection_id + '.txt', 'w',encoding='utf-8') as file:\n",
    "    for item in sorted_result:\n",
    "        file.write(\"%s %s\\n\" % (item.freq, ' '.join(item.items)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Manually choose frequent patterns and write it in the configuration file.</h1>\n",
    "<h1>Reload the configuration file:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get FP from config file\n",
    "config_data = loadConfig(base_dir + config_file)\n",
    "freq_patterns = config_data[\"collections\"][0][\"FP\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Create positive and negative samples as DataFrame</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tweets contains the frequent pattern terms will be considered as positive samples\n",
    "\n",
    "positive_tweets = (tweets\n",
    "  .rdd\n",
    "  .filter(lambda x: set(freq_patterns).issubset(x.filtered))\n",
    "  .map(lambda x : (x[0], x[1], x[2], 1.0))\n",
    "  .toDF()\n",
    "  .withColumnRenamed(\"_1\",\"id\")\n",
    "  .withColumnRenamed(\"_2\",\"text\")\n",
    "  .withColumnRenamed(\"_3\",\"filtered\")\n",
    "  .withColumnRenamed(\"_4\",\"label\"))\n",
    "\n",
    "# calculate a fraction of positive samples to extract equivalent number of negative samples\n",
    "positive_fraction = float(positive_tweets.count()) / tweets.count()\n",
    "\n",
    "# Negative samples will be randomly selected from non_positive samples\n",
    "negative_tweets = (tweets\n",
    "  .rdd\n",
    "  .filter(lambda x: not set(freq_patterns).issubset(x[2]))                   \n",
    "  .sample(False, positive_fraction, 12345)\n",
    "  .map(lambda x : (x[0], x[1], x[2], 0.0))\n",
    "  .toDF()\n",
    "  .withColumnRenamed(\"_1\",\"id\")\n",
    "  .withColumnRenamed(\"_2\",\"text\")\n",
    "  .withColumnRenamed(\"_3\",\"filtered\")\n",
    "  .withColumnRenamed(\"_4\",\"label\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Create training data by joining positive and negative samples:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data = positive_tweets.unionAll(negative_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Train LogisticRegression Classifier:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Configure an ML pipeline, which consists of the following stages: hashingTF, idf, and lr.\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"TF_features\")\n",
    "idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol=\"features\")\n",
    "pipeline1 = Pipeline(stages=[hashingTF, idf])\n",
    "\n",
    "# Fit the pipeline1 to training documents.\n",
    "model1 = pipeline1.fit(training_data)\n",
    "\n",
    "# TODO: more hyperparameter tuning is required\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "pipeline2 = Pipeline(stages=[model1, lr])\n",
    "\n",
    "# Fit the pipeline2 to training documents.\n",
    "model2 = pipeline2.fit(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Evaluating LogisticRegression model on training data:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 7.55988508975e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/ml/classification.py:207: UserWarning: weights is deprecated. Use coefficients instead.\n",
      "  warnings.warn(\"weights is deprecated. Use coefficients instead.\")\n"
     ]
    }
   ],
   "source": [
    "training_prediction = model2.transform(training_data)\n",
    "selected = training_prediction.select(\"label\", \"prediction\").rdd.map(lambda x: (x[0], x[1]))\n",
    "training_error = selected.filter(lambda (label, prediction): label != prediction).count() / float(tweets.count())\n",
    "print(\"Training Error = \" + str(training_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> TODO: More classifier to be added:</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Prepare Test Data:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_data = (tweets\n",
    "                .rdd\n",
    "                .map(lambda x: Row(id=x[0], filtered=x[2]))\n",
    "                .toDF())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=u'602-581046908535353344', prediction=0.0)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction = model2.transform(testing_data)\n",
    "selected = prediction.select(\"id\", \"prediction\")\n",
    "selected.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>prediction DataFrame contians \"prediction\" column to be filled in Hbase</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
