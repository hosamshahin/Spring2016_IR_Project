{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs, re, json, os, time\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.mllib.fpm import FPGrowth\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, IDF, StopWordsRemover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Create Spark and SQL context:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"Text Classifier\")\n",
    "# if not sc:\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Load Configuratoin File</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_config(config_file):\n",
    "    \"\"\"\n",
    "    Load collection configuration file.\n",
    "    \"\"\"\n",
    "    with open(config_file) as data_file:    \n",
    "        config_data = json.load(data_file)\n",
    "    return config_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Parse Tweets to tweet_id and tweet_text</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_tweet(line):\n",
    "    \"\"\"\n",
    "    Parses a tweet record having the following format collectionId-tweetId<\\t>tweetString\n",
    "    \"\"\"\n",
    "    fields = line.strip().split(\"\\t\")\n",
    "    if len(fields) == 2:\n",
    "        # The following regex just strips of an URL (not just http), any punctuations, \n",
    "        # User Names or Any non alphanumeric characters\n",
    "        # http://goo.gl/J8ZxDT\n",
    "        text = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",fields[1]).strip()\n",
    "        # remove terms <= 2 characters\n",
    "        text = ' '.join(filter(lambda x: len(x) > 2, text.split(\" \")))\n",
    "        # return tuple of (collectionId-tweetId, text)\n",
    "        return (fields[0], text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Load tweets from file into DataFrame:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Load_tweets(collection_id):\n",
    "    tweets_file = os.path.join(base_dir , data_dir , \"z_\" + collection_id)\n",
    "    print(\"Loading \" + tweets_file) \n",
    "    if not os.path.isdir(tweets_file):\n",
    "        print(tweets_file + \" folder doesn't exist.\")\n",
    "        return False\n",
    "    tweets = sc.textFile(tweets_file) \\\n",
    "              .map(parse_tweet) \\\n",
    "              .filter(lambda x: x is not None) \\\n",
    "              .map(lambda x: Row(id=x[0], text=x[1])) \\\n",
    "              .toDF() \\\n",
    "              .cache()\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h1>Tokenize and remove stop words from tweet text:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_tweets(tweets):\n",
    "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "    tweets = tokenizer.transform(tweets)\n",
    "    remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "    tweets = remover.transform(tweets)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Save unique tokens:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_unique_token(tweets):\n",
    "    tweets = (tweets\n",
    "      .rdd\n",
    "      .map(lambda x : (x.id, x.text, list(set(filter(None, x.filtered)))))\n",
    "      .toDF()\n",
    "      .withColumnRenamed(\"_1\",\"id\")\n",
    "      .withColumnRenamed(\"_2\",\"text\")\n",
    "      .withColumnRenamed(\"_3\",\"filtered\")).cache()\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Run Frequent Pattern Mining algorithm and save to output file:</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_FPM(tweets, collection):\n",
    "    model = FPGrowth.train(tweets.select(\"filtered\").rdd.map(lambda x: x[0]), minSupport=0.02)\n",
    "    result = sorted(model.freqItemsets().collect(), reverse=True)\n",
    "    # sort the result in reverse order\n",
    "    sorted_result = sorted(result, key=lambda item: int(item.freq), reverse=True)\n",
    "\n",
    "    # save output to file\n",
    "    with codecs.open(FP_dir + time.strftime(\"%Y%m%d-%H%M%S\") + '_'\n",
    "                            + collection[\"Id\"] + '_' \n",
    "                            + collection[\"name\"] + '.txt', 'w',encoding='utf-8') as file:\n",
    "        for item in sorted_result:\n",
    "            file.write(\"%s %s\\n\" % (item.freq, ' '.join(item.items)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Global Variables</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_dir = \"/home/hshahin/Spring2016_IR_Project/data/\"\n",
    "data_dir = \"small_data\"\n",
    "predictions_dir = os.path.join(base_dir , data_dir, \"predictions\")\n",
    "FP_dir = base_dir + \"FPGrowth/\"\n",
    "config_file = \"collections_config.json\"\n",
    "config_data = load_config(os.path.join(base_dir , config_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Phase I: Run FPM to all data sets</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /home/hshahin/Spring2016_IR_Project/data/small_data/z_602\n",
      "Loading /home/hshahin/Spring2016_IR_Project/data/small_data/z_541\n",
      "Loading /home/hshahin/Spring2016_IR_Project/data/small_data/z_668\n",
      "Loading /home/hshahin/Spring2016_IR_Project/data/small_data/z_700\n",
      "Loading /home/hshahin/Spring2016_IR_Project/data/small_data/z_686\n",
      "Loading /home/hshahin/Spring2016_IR_Project/data/small_data/z_694\n",
      "Loading /home/hshahin/Spring2016_IR_Project/data/small_data/z_532\n",
      "/home/hshahin/Spring2016_IR_Project/data/small_data/z_532 folder doesn't exist.\n"
     ]
    }
   ],
   "source": [
    "for x in config_data[\"collections\"]:\n",
    "    tweets = Load_tweets(x[\"Id\"])\n",
    "    if tweets:\n",
    "        tweets = preprocess_tweets(tweets)\n",
    "        tweets = save_unique_token(tweets)\n",
    "        run_FPM(tweets, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Manually choose frequent patterns and write them in the configuration file.</h1>\n",
    "<h1>Reload the configuration file:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get FP from config file\n",
    "config_data = load_config(os.path.join(base_dir , config_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Create training data of positive and negative samples as DataFrame</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_training_data(tweets, freq_patterns):\n",
    "    # Tweets contains the frequent pattern terms will be considered as positive samples\n",
    "    positive_tweets = (tweets\n",
    "      .rdd\n",
    "      .filter(lambda x: set(freq_patterns).issubset(x.filtered))\n",
    "      .map(lambda x : (x[0], x[1], x[2], 1.0))\n",
    "      .toDF()\n",
    "      .withColumnRenamed(\"_1\",\"id\")\n",
    "      .withColumnRenamed(\"_2\",\"text\")\n",
    "      .withColumnRenamed(\"_3\",\"filtered\")\n",
    "      .withColumnRenamed(\"_4\",\"label\"))\n",
    "\n",
    "    # calculate a fraction of positive samples to extract equivalent number of negative samples\n",
    "    positive_fraction = float(positive_tweets.count()) / tweets.count()\n",
    "\n",
    "    # Negative samples will be randomly selected from non_positive samples\n",
    "    negative_tweets = (tweets\n",
    "      .rdd\n",
    "      .filter(lambda x: not set(freq_patterns).issubset(x[2]))                   \n",
    "      .sample(False, positive_fraction, 12345)\n",
    "      .map(lambda x : (x[0], x[1], x[2], 0.0))\n",
    "      .toDF()\n",
    "      .withColumnRenamed(\"_1\",\"id\")\n",
    "      .withColumnRenamed(\"_2\",\"text\")\n",
    "      .withColumnRenamed(\"_3\",\"filtered\")\n",
    "      .withColumnRenamed(\"_4\",\"label\"))\n",
    "    training_data = positive_tweets.unionAll(negative_tweets)\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Train LogisticRegression Classifier:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_lg(training_data):\n",
    "    # Configure an ML pipeline, which consists of the following stages: hashingTF, idf, and lr.\n",
    "    hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"TF_features\")\n",
    "    idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol=\"features\")\n",
    "    pipeline1 = Pipeline(stages=[hashingTF, idf])\n",
    "\n",
    "    # Fit the pipeline1 to training documents.\n",
    "    model1 = pipeline1.fit(training_data)\n",
    "\n",
    "    # TODO: more hyperparameter tuning is required\n",
    "    lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "    pipeline2 = Pipeline(stages=[model1, lr])\n",
    "\n",
    "    # Fit the pipeline2 to training documents.\n",
    "    model2 = pipeline2.fit(training_data)\n",
    "    return model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Evaluating LogisticRegression model on training data:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_training_score_lg(lg_model, training_data):\n",
    "    training_prediction = lg_model.transform(training_data)\n",
    "    selected = training_prediction.select(\"label\", \"prediction\").rdd.map(lambda x: (x[0], x[1]))\n",
    "    training_error = selected.filter(lambda (label, prediction): label != prediction).count() / float(tweets.count())\n",
    "    print(\"Training Error = \" + str(training_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Prepare testing data:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_testing_data(tweets):\n",
    "    testing_data = (tweets\n",
    "                    .rdd\n",
    "                    .map(lambda x: Row(id=x[0], filtered=x[2]))\n",
    "                    .toDF())\n",
    "    return testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lg_prediction(lg_model, testing_data, collection):\n",
    "    # Perfom predictions on test documents and save columns of interest to a file.\n",
    "    prediction = lg_model.transform(testing_data)\n",
    "    selected = prediction.select(\"id\", \"prediction\")\n",
    "    prediction_path = os.path.join(predictions_dir , time.strftime(\"%Y%m%d-%H%M%S\") + '_'\n",
    "                            + collection[\"Id\"] + '_' \n",
    "                            + collection[\"name\"])\n",
    "    print(prediction_path)\n",
    "    def saveData(data):\n",
    "        with open(prediction_path, 'a') as f:\n",
    "            f.write(data.id+\"\\t\"+str(data.prediction)+\"\\n\")\n",
    "    selected.foreach(saveData)    \n",
    "    # selected.rdd.saveAsTextFile(prediction_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Phase II: Train classifier and perform prediction</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /home/hshahin/Spring2016_IR_Project/data/small_data/z_602\n",
      "Training Error = 7.55988508975e-05\n",
      "/home/hshahin/Spring2016_IR_Project/data/small_data/predictions/20160331-171230_602_Germanwings\n",
      "Loading /home/hshahin/Spring2016_IR_Project/data/small_data/z_541\n",
      "Training Error = 5.90510496324e-05\n",
      "/home/hshahin/Spring2016_IR_Project/data/small_data/predictions/20160331-171248_541_NAACPBombing\n",
      "Loading /home/hshahin/Spring2016_IR_Project/data/small_data/z_668\n",
      "Training Error = 0.0\n",
      "/home/hshahin/Spring2016_IR_Project/data/small_data/predictions/20160331-171301_668_houstonflood\n",
      "Loading /home/hshahin/Spring2016_IR_Project/data/small_data/z_700\n",
      "Training Error = 0.0\n",
      "/home/hshahin/Spring2016_IR_Project/data/small_data/predictions/20160331-171313_700_wdbj7 shooting \n",
      "Loading /home/hshahin/Spring2016_IR_Project/data/small_data/z_686\n",
      "Training Error = 0.0\n",
      "/home/hshahin/Spring2016_IR_Project/data/small_data/predictions/20160331-171350_686_Obamacare\n",
      "Loading /home/hshahin/Spring2016_IR_Project/data/small_data/z_694\n",
      "Training Error = 0.000879532429212\n",
      "/home/hshahin/Spring2016_IR_Project/data/small_data/predictions/20160331-171409_694_4thofJuly\n",
      "Loading /home/hshahin/Spring2016_IR_Project/data/small_data/z_532\n",
      "/home/hshahin/Spring2016_IR_Project/data/small_data/z_532 folder doesn't exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/ml/classification.py:207: UserWarning: weights is deprecated. Use coefficients instead.\n",
      "  warnings.warn(\"weights is deprecated. Use coefficients instead.\")\n"
     ]
    }
   ],
   "source": [
    "for x in config_data[\"collections\"]:\n",
    "    tweets = Load_tweets(x[\"Id\"])\n",
    "    if tweets:\n",
    "        freq_patterns = x[\"FP\"]\n",
    "        tweets = preprocess_tweets(tweets)\n",
    "        tweets = save_unique_token(tweets)\n",
    "        training_data = create_training_data(tweets, freq_patterns)\n",
    "        lg_model = train_lg(training_data)\n",
    "        get_training_score_lg(lg_model, training_data)\n",
    "        testing_data = create_testing_data(tweets)\n",
    "        lg_prediction(lg_model, testing_data, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>prediction DataFrame contians \"prediction\" column to be filled in Hbase</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction = model2.transform(testing_data)\n",
    "selected = prediction.select(\"id\", \"prediction\")\n",
    "def saveData(data):\n",
    "    with open(base_dir+'FPGrowth/700_classified.txt', 'a') as f:\n",
    "        f.write(data.id+\"\\t\"+str(data.prediction)+\"\\n\")\n",
    "selected.foreach(saveData)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
